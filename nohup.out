Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.
Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.
Symbol nvrtcGetCUBIN not found in /usr/local/cuda/lib64/libnvrtc.so
Symbol nvrtcGetCUBINSize not found in /usr/local/cuda/lib64/libnvrtc.so
web_demo.py:85: GradioDeprecationWarning: The `style` method is deprecated. Please set these arguments in the constructor instead.
  user_input = gr.Textbox(show_label=False, placeholder="Input...", lines=10).style(
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 56, in generator_context
    response = gen.send(request)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 56, in generator_context
    response = gen.send(request)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 648, in forward
    mlp_output = self.mlp(mlp_input)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 545, in forward
    intermediate_parallel = self.dense_h_to_4h(hidden_states)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 391, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 56, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 280, in extract_weight_to_half
    out = torch.empty(n, m * (8 // source_bit_width), dtype=torch.half, device="cuda")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 5.70 GiB already allocated; 111.54 MiB free; 6.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 627, in forward
    attention_outputs = self.attention(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 445, in forward
    mixed_raw_layer = self.query_key_value(hidden_states)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 391, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 56, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 280, in extract_weight_to_half
    out = torch.empty(n, m * (8 // source_bit_width), dtype=torch.half, device="cuda")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 8.00 GiB total capacity; 5.72 GiB already allocated; 111.54 MiB free; 6.03 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 627, in forward
    attention_outputs = self.attention(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 445, in forward
    mixed_raw_layer = self.query_key_value(hidden_states)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 391, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 56, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 280, in extract_weight_to_half
    out = torch.empty(n, m * (8 // source_bit_width), dtype=torch.half, device="cuda")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 8.00 GiB total capacity; 5.72 GiB already allocated; 109.54 MiB free; 6.04 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 56, in generator_context
    response = gen.send(request)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 56, in generator_context
    response = gen.send(request)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 648, in forward
    mlp_output = self.mlp(mlp_input)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 545, in forward
    intermediate_parallel = self.dense_h_to_4h(hidden_states)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 391, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 56, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 280, in extract_weight_to_half
    out = torch.empty(n, m * (8 // source_bit_width), dtype=torch.half, device="cuda")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 5.69 GiB already allocated; 163.54 MiB free; 5.98 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 627, in forward
    attention_outputs = self.attention(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 474, in forward
    context_layer, present, attention_probs = attention_fn(
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 308, in attention_fn
    attention_scores = attention_scores.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 164.00 MiB (GPU 0; 8.00 GiB total capacity; 5.86 GiB already allocated; 53.54 MiB free; 6.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 56, in generator_context
    response = gen.send(request)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 56, in generator_context
    response = gen.send(request)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 648, in forward
    mlp_output = self.mlp(mlp_input)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 545, in forward
    intermediate_parallel = self.dense_h_to_4h(hidden_states)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 391, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 56, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 280, in extract_weight_to_half
    out = torch.empty(n, m * (8 // source_bit_width), dtype=torch.half, device="cuda")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 128.00 MiB (GPU 0; 8.00 GiB total capacity; 5.73 GiB already allocated; 153.54 MiB free; 5.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 627, in forward
    attention_outputs = self.attention(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 474, in forward
    context_layer, present, attention_probs = attention_fn(
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 308, in attention_fn
    attention_scores = attention_scores.float()
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 170.00 MiB (GPU 0; 8.00 GiB total capacity; 5.91 GiB already allocated; 57.54 MiB free; 6.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 807, in run
    result = context.run(func, *args)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 484, in run_sync_iterator_async
    return next(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 853, in gen_wrapper
    yield from f(*args, **kwargs)
  File "web_demo.py", line 63, in predict
    for response, history in model.stream_chat(tokenizer, input, history, max_length=max_length, top_p=top_p,
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1311, in stream_chat
    for outputs in self.stream_generate(**inputs, **gen_kwargs):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/utils/_contextlib.py", line 35, in generator_context
    response = gen.send(None)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1388, in stream_generate
    outputs = self(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 1190, in forward
    transformer_outputs = self.transformer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 996, in forward
    layer_ret = layer(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 627, in forward
    attention_outputs = self.attention(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/modeling_chatglm.py", line 445, in forward
    mixed_raw_layer = self.query_key_value(hidden_states)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 391, in forward
    output = W8A16Linear.apply(input, self.weight, self.weight_scale, self.weight_bit_width)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/torch/autograd/function.py", line 506, in apply
    return super().apply(*args, **kwargs)  # type: ignore[misc]
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 56, in forward
    weight = extract_weight_to_half(quant_w, scale_w, weight_bit_width)
  File "/root/.cache/huggingface/modules/transformers_modules/chatglm-6b-int4/quantization.py", line 280, in extract_weight_to_half
    out = torch.empty(n, m * (8 // source_bit_width), dtype=torch.half, device="cuda")
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 8.00 GiB total capacity; 5.93 GiB already allocated; 57.54 MiB free; 6.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
Traceback (most recent call last):
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/routes.py", line 523, in run_predict
    output = await app.get_blocks().process_api(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1437, in process_api
    result = await self.call_function(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/blocks.py", line 1123, in call_function
    prediction = await utils.async_iteration(iterator)
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 508, in async_iteration
    return await iterator.__anext__()
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/gradio/utils.py", line 501, in __anext__
    return await anyio.to_thread.run_sync(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/to_thread.py", line 33, in run_sync
    return await get_asynclib().run_sync_in_worker_thread(
  File "/data/software/anaconda3/envs/chatglm/lib/python3.8/site-packages/anyio/_backends/_asyncio.py", line 877, in run_sync_in_worker_thread
    return await future
